import { createClient } from '@supabase/supabase-js'
import { GoogleAuth } from 'google-auth-library'
import { DocumentProcessorServiceClient } from '@google-cloud/documentai'
import { PredictionServiceClient } from '@google-cloud/aiplatform'
import { DLP } from '@google-cloud/dlp'

// Advanced Document Processing Service with Google Cloud Document AI
export class AdvancedDocumentProcessor {
  private documentAI: DocumentProcessorServiceClient
  private vertexAI: PredictionServiceClient
  private dlp: DLP
  private supabase: any
  
  constructor() {
    // Initialize Google Cloud clients
    const auth = new GoogleAuth({
      scopes: ['https://www.googleapis.com/auth/cloud-platform']
    })
    
    this.documentAI = new DocumentProcessorServiceClient({ auth })
    this.vertexAI = new PredictionServiceClient({ auth })
    this.dlp = new DLP({ auth })
    
    this.supabase = createClient(
      Deno.env.get('SUPABASE_URL')!,
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    )
  }

  /**
   * Enhanced Document AI Form Parser for Resumes and Job Descriptions
   * Supports PDF, DOCX, DOC, RTF, TXT with enterprise-grade accuracy
   */
  async parseDocument(fileBuffer: Uint8Array, mimeType: string, documentType: 'resume' | 'job_description'): Promise<ParsedDocument> {
    try {
      const processorName = documentType === 'resume' 
        ? `projects/${Deno.env.get('GOOGLE_CLOUD_PROJECT_ID')}/locations/us/processors/${Deno.env.get('RESUME_PROCESSOR_ID')}`
        : `projects/${Deno.env.get('GOOGLE_CLOUD_PROJECT_ID')}/locations/us/processors/${Deno.env.get('JD_PROCESSOR_ID')}`

      // Process document with Google Cloud Document AI
      const [result] = await this.documentAI.processDocument({
        name: processorName,
        rawDocument: {
          content: Buffer.from(fileBuffer).toString('base64'),
          mimeType: mimeType
        }
      })

      // Extract structured data using Document AI insights
      const extractedData = this.extractStructuredData(result.document, documentType)
      
      // Apply PII protection before processing
      const protectedData = await this.applyPIIProtection(extractedData)
      
      return protectedData

    } catch (error) {
      console.error('Document AI parsing error:', error)
      throw new Error(`Failed to parse ${documentType}: ${error.message}`)
    }
  }

  /**
   * Extract structured data from Document AI results
   */
  private extractStructuredData(document: any, type: 'resume' | 'job_description'): ParsedDocument {
    const entities = document.entities || []
    const text = document.text || ''
    
    if (type === 'resume') {
      return {
        type: 'resume',
        rawText: text,
        personalInfo: this.extractPersonalInfo(entities, text),
        workExperience: this.extractWorkExperience(entities, text),
        education: this.extractEducation(entities, text),
        skills: this.extractSkills(entities, text),
        certifications: this.extractCertifications(entities, text),
        projects: this.extractProjects(entities, text),
        languages: this.extractLanguages(entities, text),
        achievements: this.extractAchievements(entities, text)
      }
    } else {
      return {
        type: 'job_description',
        rawText: text,
        jobTitle: this.extractJobTitle(entities, text),
        company: this.extractCompany(entities, text),
        location: this.extractLocation(entities, text),
        requirements: this.extractRequirements(entities, text),
        responsibilities: this.extractResponsibilities(entities, text),
        qualifications: this.extractQualifications(entities, text),
        skills: this.extractRequiredSkills(entities, text),
        benefits: this.extractBenefits(entities, text),
        salary: this.extractSalary(entities, text)
      }
    }
  }

  /**
   * Advanced PII Protection using Google Cloud DLP
   */
  private async applyPIIProtection(document: ParsedDocument): Promise<ParsedDocument> {
    try {
      const projectPath = this.dlp.projectPath(Deno.env.get('GOOGLE_CLOUD_PROJECT_ID')!)
      
      // Define PII types to protect
      const infoTypes = [
        { name: 'EMAIL_ADDRESS' },
        { name: 'PHONE_NUMBER' },
        { name: 'PERSON_NAME' },
        { name: 'CREDIT_CARD_NUMBER' },
        { name: 'US_SOCIAL_SECURITY_NUMBER' },
        { name: 'DATE_OF_BIRTH' },
        { name: 'US_DRIVERS_LICENSE_NUMBER' }
      ]

      // Apply tokenization to sensitive fields
      const protectedDocument = { ...document }
      
      if (document.type === 'resume' && document.personalInfo) {
        // Tokenize personal information while preserving structure
        const [emailResult] = await this.dlp.deidentifyContent({
          parent: projectPath,
          deidentifyConfig: {
            infoTypeTransformations: {
              transformations: [{
                infoTypes: [{ name: 'EMAIL_ADDRESS' }],
                primitiveTransformation: {
                  cryptoReplaceFfxFpeConfig: {
                    cryptoKey: {
                      kmsWrapped: {
                        wrappedKey: Deno.env.get('DLP_WRAPPED_KEY')!,
                        cryptoKeyName: Deno.env.get('DLP_CRYPTO_KEY_NAME')!
                      }
                    },
                    commonAlphabet: 'ALPHA_NUMERIC'
                  }
                }
              }]
            }
          },
          item: { value: document.personalInfo.email || '' }
        })

        protectedDocument.personalInfo = {
          ...document.personalInfo,
          email: emailResult.item?.value || document.personalInfo.email,
          isProtected: true
        }
      }

      return protectedDocument

    } catch (error) {
      console.error('PII protection error:', error)
      // Return original document if PII protection fails (non-blocking)
      return document
    }
  }

  /**
   * Generate high-quality embeddings using Vertex AI Text Embeddings
   */
  async generateVertexEmbeddings(texts: string[]): Promise<EmbeddingResult[]> {
    try {
      const endpoint = `projects/${Deno.env.get('GOOGLE_CLOUD_PROJECT_ID')}/locations/us-central1/publishers/google/models/textembedding-gecko@003`
      
      const instances = texts.map(text => ({
        content: text,
        task_type: 'SEMANTIC_SIMILARITY'
      }))

      const request = {
        endpoint,
        instances,
        parameters: {
          outputDimensionality: 768, // High-dimensional embeddings for better accuracy
          autoTruncate: true
        }
      }

      const [response] = await this.vertexAI.predict(request)
      
      return response.predictions?.map((prediction: any, index: number) => ({
        text: texts[index],
        embedding: prediction.embeddings?.values || [],
        dimension: prediction.embeddings?.values?.length || 0
      })) || []

    } catch (error) {
      console.error('Vertex AI embedding error:', error)
      throw new Error(`Failed to generate embeddings: ${error.message}`)
    }
  }

  /**
   * Advanced semantic search with optimized pgvector queries
   */
  async performSemanticSearch(
    queryEmbedding: number[],
    documentType: 'resume' | 'job_description',
    userId?: string,
    limit: number = 10,
    threshold: number = 0.7
  ): Promise<SemanticSearchResult[]> {
    try {
      let query = this.supabase
        .rpc('advanced_vector_search', {
          query_embedding: queryEmbedding,
          similarity_threshold: threshold,
          match_count: limit,
          document_type: documentType
        })

      if (userId) {
        query = query.eq('user_id', userId)
      }

      const { data, error } = await query

      if (error) throw error

      return data?.map((result: any) => ({
        id: result.id,
        content: result.content,
        similarity: result.similarity,
        metadata: result.metadata,
        documentType: result.document_type,
        createdAt: result.created_at
      })) || []

    } catch (error) {
      console.error('Semantic search error:', error)
      throw new Error(`Semantic search failed: ${error.message}`)
    }
  }

  /**
   * Store document with embeddings in optimized vector database
   */
  async storeDocumentWithEmbeddings(
    document: ParsedDocument,
    userId: string,
    embeddings: EmbeddingResult[]
  ): Promise<string> {
    try {
      // Store main document
      const { data: documentData, error: docError } = await this.supabase
        .from('enhanced_documents')
        .insert({
          user_id: userId,
          document_type: document.type,
          content: document,
          raw_text: document.rawText,
          processing_metadata: {
            parser_version: 'document-ai-v1',
            pii_protected: document.personalInfo?.isProtected || false,
            embedding_model: 'textembedding-gecko@003',
            processed_at: new Date().toISOString()
          }
        })
        .select('id')
        .single()

      if (docError) throw docError

      // Store embeddings with optimized indexing
      const embeddingInserts = embeddings.map(emb => ({
        document_id: documentData.id,
        user_id: userId,
        content_section: emb.text,
        embedding: emb.embedding,
        document_type: document.type,
        metadata: {
          dimension: emb.dimension,
          section_type: this.inferSectionType(emb.text, document.type)
        }
      }))

      const { error: embError } = await this.supabase
        .from('document_embeddings')
        .insert(embeddingInserts)

      if (embError) throw embError

      return documentData.id

    } catch (error) {
      console.error('Document storage error:', error)
      throw new Error(`Failed to store document: ${error.message}`)
    }
  }

  // Private helper methods for entity extraction
  private extractPersonalInfo(entities: any[], text: string): PersonalInfo {
    // Advanced entity extraction logic using Document AI results
    return {
      name: this.findEntityValue(entities, 'PERSON_NAME') || this.extractNameFromText(text),
      email: this.findEntityValue(entities, 'EMAIL_ADDRESS') || this.extractEmailFromText(text),
      phone: this.findEntityValue(entities, 'PHONE_NUMBER') || this.extractPhoneFromText(text),
      location: this.findEntityValue(entities, 'LOCATION') || this.extractLocationFromText(text),
      linkedin: this.extractLinkedInFromText(text),
      portfolio: this.extractPortfolioFromText(text)
    }
  }

  private extractWorkExperience(entities: any[], text: string): WorkExperience[] {
    // Extract work experience using Document AI entity detection
    const experiences: WorkExperience[] = []
    const workSections = this.findWorkExperienceSections(text)
    
    workSections.forEach(section => {
      experiences.push({
        company: this.extractCompanyFromSection(section),
        position: this.extractPositionFromSection(section),
        duration: this.extractDurationFromSection(section),
        location: this.extractLocationFromSection(section),
        responsibilities: this.extractResponsibilitiesFromSection(section),
        achievements: this.extractAchievementsFromSection(section)
      })
    })

    return experiences
  }

  private extractSkills(entities: any[], text: string): string[] {
    // Extract skills using both entity detection and pattern matching
    const skillEntities = entities.filter(e => e.type === 'SKILL' || e.type === 'TECHNOLOGY')
    const extractedSkills = skillEntities.map(e => e.mentionText)
    
    // Also use pattern matching for common skill formats
    const skillPatterns = [
      /(?:Skills?|Technologies?|Programming Languages?|Tools?)[:\s]*(.*?)(?:\n\n|$)/gi,
      /â€¢\s*([A-Za-z+#.]+(?:\s+[A-Za-z+#.]+)*)/g
    ]
    
    skillPatterns.forEach(pattern => {
      const matches = text.match(pattern)
      if (matches) {
        extractedSkills.push(...matches)
      }
    })

    return [...new Set(extractedSkills.map(skill => skill.trim()))]
  }

  private inferSectionType(text: string, documentType: string): string {
    // Infer what section of the document this text belongs to
    const lowerText = text.toLowerCase()
    
    if (documentType === 'resume') {
      if (lowerText.includes('experience') || lowerText.includes('work') || lowerText.includes('employment')) {
        return 'work_experience'
      } else if (lowerText.includes('education') || lowerText.includes('university') || lowerText.includes('degree')) {
        return 'education'
      } else if (lowerText.includes('skill') || lowerText.includes('technology') || lowerText.includes('programming')) {
        return 'skills'
      } else if (lowerText.includes('project') || lowerText.includes('portfolio')) {
        return 'projects'
      }
    } else if (documentType === 'job_description') {
      if (lowerText.includes('requirement') || lowerText.includes('qualification')) {
        return 'requirements'
      } else if (lowerText.includes('responsibility') || lowerText.includes('duties')) {
        return 'responsibilities'
      } else if (lowerText.includes('benefit') || lowerText.includes('compensation')) {
        return 'benefits'
      }
    }
    
    return 'general'
  }

  private findEntityValue(entities: any[], entityType: string): string | null {
    const entity = entities.find(e => e.type === entityType)
    return entity?.mentionText || null
  }

  // Additional extraction methods would be implemented here...
  private extractNameFromText(text: string): string | null {
    // Fallback name extraction using regex patterns
    const namePattern = /^([A-Z][a-z]+\s+[A-Z][a-z]+)/m
    const match = text.match(namePattern)
    return match ? match[1] : null
  }

  private extractEmailFromText(text: string): string | null {
    const emailPattern = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/
    const match = text.match(emailPattern)
    return match ? match[0] : null
  }

  private extractPhoneFromText(text: string): string | null {
    const phonePattern = /(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})/
    const match = text.match(phonePattern)
    return match ? match[0] : null
  }

  // ... other extraction methods
}

// Type definitions for enhanced document processing
export interface ParsedDocument {
  type: 'resume' | 'job_description'
  rawText: string
  personalInfo?: PersonalInfo
  workExperience?: WorkExperience[]
  education?: Education[]
  skills?: string[]
  certifications?: string[]
  projects?: Project[]
  languages?: string[]
  achievements?: string[]
  // Job description specific fields
  jobTitle?: string
  company?: string
  location?: string
  requirements?: string[]
  responsibilities?: string[]
  qualifications?: string[]
  benefits?: string[]
  salary?: string
}

export interface PersonalInfo {
  name?: string
  email?: string
  phone?: string
  location?: string
  linkedin?: string
  portfolio?: string
  isProtected?: boolean
}

export interface WorkExperience {
  company: string
  position: string
  duration: string
  location?: string
  responsibilities: string[]
  achievements: string[]
}

export interface Education {
  institution: string
  degree: string
  field: string
  graduationYear?: string
  gpa?: string
}

export interface Project {
  name: string
  description: string
  technologies: string[]
  url?: string
}

export interface EmbeddingResult {
  text: string
  embedding: number[]
  dimension: number
}

export interface SemanticSearchResult {
  id: string
  content: string
  similarity: number
  metadata: any
  documentType: string
  createdAt: string
}
